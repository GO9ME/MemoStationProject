import json
import faiss
import numpy as np
import time
import os
import re
import nltk


from pathlib import Path
from dotenv import load_dotenv
from typing import List, Dict, Optional, Tuple
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm
from openai import OpenAI


# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ
try:
    nltk.download("punkt", quiet=True)
    nltk.download("stopwords", quiet=True)
except:
    pass


class RAGBlogRecommender:
    def __init__(
        self,
        folder_path: str = "new_naver_crawl/crawl_new_data2",
        # folder_path: str,
        embedding_model: str = "text-embedding-3-small",
        index_path: str = "faiss_index_basic.faiss",
        embed_path: str = "embeddings_basic.npy",
        meta_path: str = "doc_metadata_basic.json",
        api_key: str = None,
    ):

        self.folder_path = folder_path
        self.embedding_model = embedding_model
        self.index_path = index_path
        self.embed_path = embed_path
        self.meta_path = meta_path
        # self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

        self.doc_texts = []
        self.doc_ids = []
        self.doc_meta = []
        self.index = None

        """
        RAG ê¸°ë°˜ ë¸”ë¡œê·¸ ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”

        Args:
            blog_data_path: ë¸”ë¡œê·¸ í¬ë¡¤ë§ ë°ì´í„° ê²½ë¡œ
            api_key: OpenAI API í‚¤
        """
        self.blog_data_path = Path(folder_path)
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")

        print(os.getenv("OPENAI_API_KEY"))

        if not self.api_key:
            raise ValueError("OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        self.openai_client = OpenAI(api_key=self.api_key)

        # í•œêµ­ì–´/ì˜ì–´ ë¶ˆìš©ì–´ ì„¤ì •
        self.stop_words = set(stopwords.words("english"))
        try:
            self.stop_words.update(stopwords.words("korean"))
        except:
            pass

        # TF-IDF ë²¡í„°ë¼ì´ì €
        self.vectorizer = TfidfVectorizer(
            max_features=5000, stop_words=list(self.stop_words), ngram_range=(1, 2)
        )

        # ë¸”ë¡œê·¸ ë°ì´í„° ì €ì¥ì†Œ
        self.blog_data = []
        self.blog_embeddings = None
        self.blog_tfidf_matrix = None

        # ì¹´í…Œê³ ë¦¬ ë§¤í•‘
        self.category_mapping = {
            "ì§€ì‹ë™í–¥": [
                "êµìœ¡í•™ë¬¸",
                "ì–´í•™ì™¸êµ­ì–´",
                "ë¹„ì¦ˆë‹ˆìŠ¤ê²½ì œ",
                "ê±´ê°•ì˜í•™",
                "ì‚¬íšŒì •ì¹˜",
                "ITì»´í“¨í„°",
            ],
            "ì·¨ë¯¸ì—¬ê°€ì—¬í–‰": [
                "ë§›ì§‘",
                "ì„¸ê³„ì—¬í–‰",
                "êµ­ë‚´ì—¬í–‰",
                "ì·¨ë¯¸",
                "ìë™ì°¨",
                "ì‚¬ì§„",
                "ìŠ¤í¬ì¸ ",
                "ê²Œì„",
            ],
            "ìƒí™œë…¸í•˜ìš°ì‡¼í•‘": [
                "ì›ì˜ˆì¬ë°°",
                "ìƒí’ˆë¦¬ë·°",
                "ìš”ë¦¬ë ˆì‹œí”¼",
                "ì¸í…Œë¦¬ì–´DIY",
                "íŒ¨ì…˜ë¯¸ìš©",
                "ì¢‹ì€ê¸€ì´ë¯¸ì§€",
                "ë°˜ë ¤ë™ë¬¼",
                "ìœ¡ì•„ê²°í˜¼",
                "ì¼ìƒìƒê°",
            ],
            "ì—”í„°í…Œì¸ë¨¼íŠ¸ì˜ˆìˆ ": [
                "ë°©ì†¡",
                "ë§Œí™”ì• ë‹ˆ",
                "ìŠ¤íƒ€ì—°ì˜ˆì¸",
                "ë“œë¼ë§ˆ",
                "ìŒì•…",
                "ê³µì—°ì „ì‹œ",
                "ë¯¸ìˆ ë””ìì¸",
                "ì˜í™”",
                "ë¬¸í•™ì±…",
            ],
        }

    def preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if not text:
            return ""

        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r"<[^>]+>", "", text)

        # íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±ë§Œ ìœ ì§€)
        text = re.sub(r"[^\w\sê°€-í£]", " ", text)

        # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = re.sub(r"\s+", " ", text)

        # ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()

        return text

    def extract_json_prompt(self, text: str) -> str:
        """
        LLM ì‘ë‹µ ë¬¸ìì—´ì—ì„œ JSON ë¸”ëŸ­ì„ ì¶”ì¶œí•˜ê³  'prompt' ê°’ë§Œ ë°˜í™˜
        """
        # ì½”ë“œë¸”ëŸ­ ì œê±°: ```json ë˜ëŠ” ``` ìœ¼ë¡œ ê°ì‹¸ì§„ ë¶€ë¶„ ì‚­ì œ
        cleaned = re.sub(
            r"^```json|^```|```$", "", text.strip(), flags=re.MULTILINE
        ).strip()

        # JSON íŒŒì‹±
        try:
            data = json.loads(cleaned)
            return data.get("prompt", "")
        except json.JSONDecodeError as e:
            print("âŒ JSON íŒŒì‹± ì˜¤ë¥˜:", e)
            print("ì…ë ¥ê°’:", cleaned[:200])
            return ""

    def categorize_memo(self, memo_text: str, n: int = 2) -> str:
        # LLM ë¶„ë¥˜ëŠ” ì„ íƒì ìœ¼ë¡œ ì‚¬ìš© (API ì˜¤ë¥˜ ë°©ì§€)
        try:
            prompt = f"""

ë°˜ë“œì‹œ ì•„ë˜ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ Json í˜•íƒœë¡œ ë§Œë“¤ì–´ì¤˜.

"category": "ì¶”ì²œ ì¹´í…Œê³ ë¦¬",
"user_type": "ì‚¬ìš©ìì˜ ì„±í–¥ì„ ë¶„ì„í•˜ì—¬ ë„£ì–´ì¤€ë‹¤.",
"prompt": "ì¿¼ë¦¬ ë‚´ìš©ì„ ë°°ì—´ë¡œ ë„£ì–´ì¤€ë‹¤."

### ì‚¬ìš©ì ë©”ëª¨ ë¶„ì„
{memo_text[:1000]}

### ì¶”ì²œ category ì˜ˆì‹œ
- ì§€ì‹ë™í–¥ >  ITÂ·ì»´í“¨í„° 
- ìƒí™œë…¸í•˜ìš°ì‡¼í•‘ > ì¼ìƒìƒê°

### ì‚¬ìš©ì ì„±í–¥ ì˜ˆì‹œ
- ì‹¤ìš©ì ì´ê³  ë¶„ì„ì ì¸ ê¸€ ì„ í˜¸
- AIì™€ ì§ë¬´ê°œë°œì— ê´€ì‹¬ ë§ìŒ
- ì™¸ë¡œì›€/ì†Œí†µ ë¬¸ì œë„ ëŠë¼ê³  ìˆìŒ

## ë¸”ë¡œê·¸ category:
1. ì§€ì‹ë™í–¥: ê±´ê°•ì˜í•™, êµìœ¡í•™ë¬¸, ë¹„ì¦ˆë‹ˆìŠ¤ê²½ì œ, ì‚¬íšŒì •ì¹˜, ì–´í•™ì™¸êµ­ì–´, ITÂ·ì»´í“¨í„°
2. ì·¨ë¯¸ì—¬ê°€ì—¬í–‰: ê²Œì„, êµ­ë‚´ì—¬í–‰, ë§›ì§‘, ì‚¬ì§„, ì„¸ê³„ì—¬í–‰, ìŠ¤í¬ì¸ , ìë™ì°¨, ì·¨ë¯¸
3. ìƒí™œë…¸í•˜ìš°ì‡¼í•‘: ë°˜ë ¤ë™ë¬¼, ìƒí’ˆë¦¬ë·°, ìš”ë¦¬ë ˆì‹œí”¼, ì›ì˜ˆì¬ë°°, ìœ¡ì•„ê²°í˜¼, ì¸í…Œë¦¬ì–´DIY, ì¼ìƒìƒê°, ì¢‹ì€ê¸€ì´ë¯¸ì§€, í´ì…˜ë¯¸ìš©
4. ì—”í„°í…Œì¸ë¨¼íŠ¸ì˜ˆìˆ : ê³µì—°ì „ì‹ , ë“œë¼ë§ˆ, ë§Œí™”ì• ë‹ˆ, ë¬¸í•™ì±…, ë¯¸ìˆ ë””ìì¸, ë°©ì†¡, ìŠ¤íƒ€ì—°ì˜ˆì¸, ì˜í™”, ìŒì•…


â€» ì–´í•™/í•™ìŠµ/ê³µë¶€/í‘œí˜„/ì˜ì–´/ì™¸êµ­ì–´ ê´€ë ¨ ë‹¨ì–´ê°€ í•˜ë‚˜ë¼ë„ ë‚˜ì˜¤ë©´ ë°˜ë“œì‹œ 'ì§€ì‹ë™í–¥'ìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
â€» ìŒì‹/ìš”ë¦¬/ë ˆì‹œí”¼ ê´€ë ¨ ë‹¨ì–´ê°€ ë‚˜ì˜¤ë©´ ë°˜ë“œì‹œ 'ìƒí™œë…¸í•˜ìš°ì‡¼í•‘'ìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
â€» ê²Œì„/RPG/í”Œë ˆì´ ê´€ë ¨ ë‹¨ì–´ê°€ ë‚˜ì˜¤ë©´ ë°˜ë“œì‹œ 'ì·¨ë¯¸ì—¬ê°€ì—¬í–‰'ìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
â€» ìœ„ì˜ ì¹´í…Œê³ ë¦¬ì— ëª…í™•íˆ í•´ë‹¹í•˜ì§€ ì•Šì„ ë•ŒëŠ” ê°€ì¥ ê·¼ì ‘í•œ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš”. 'ê¸°íƒ€'ëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
            """

            system_prompt = f"""
                        ë‹¹ì‹ ì€ ë©”ëª¨ ê¸°ë°˜ ì„±í–¥ ë¶„ì„ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ì¿¼ë¦¬ë¥¼ ì‘ì„±í•´ ì£¼ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê·¸ ì•„ë˜ëŠ” ì‚¬ìš©ìì˜ ìµœê·¼ ë©”ëª¨ì— ëŒ€í•´ ì‚¬ì „ ë¶„ì„ëœ í•­ëª©ì…ë‹ˆë‹¤. 
                        ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì„±í–¥ì„ ìš”ì•½í•˜ê³ , 
                        ê°€ì¥ ì ì ˆí•œ ë¸”ë¡œê·¸ ì¹´í…Œê³ ë¦¬ 1~2ê°œë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”. ë˜í•œ, ì¶”ì²œëœ ì¹´í…Œê³ ë¦¬ì™€ ë¶„ì„ëœ ì‚¬ìš©ì ì„±í–¥ì„ ë„£ì–´ ë¸”ë¡œê·¸ ì½˜í…ì¸ ë¥¼ ì €ì¥í•œ ë²¡í„°ë””ë¹„ì—ì„œ ì¶”ì²œí•  ìˆ˜ ìˆëŠ” 
                        LLMìš© ì¿¼ë¦¬ë¥¼ í•¨ê»˜ ì‘ì„±í•´ì£¼ê³  ì˜ë¯¸ëŠ” ìœ ì§€í•˜ë©´ì„œ ë‹¤ì–‘í•œ í‘œí˜„ë°©ì‹ìœ¼ë¡œ ì¬ì‘ì„±í•œ ì¿¼ë¦¬ë¥¼ {n}ê°œ ë§Œë“¤ì–´ì¤˜. 
                        LLMìš© ì¿¼ë¦¬ì—ëŠ” ì¶”ì²œ categoryë„ í¬í•¨í•´ì¤˜, 'ê¸°íƒ€' ì¹´í…Œê³ ë¦¬ëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
            """
            print(f"[INFO]system_prompt : ==== {system_prompt}")
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system",
                        "content": system_prompt,
                    },
                    {"role": "user", "content": prompt},
                ],
                temperature=0.3,
            )

            llm_category = response.choices[0].message.content.strip().strip('"')
            # if llm_category != "ê¸°íƒ€":
            #     return llm_category
            return llm_category

        except Exception as e:
            print(f"LLM ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # LLM ì˜¤ë¥˜ ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ ê²°ê³¼ ì‚¬ìš©

    def get_openai_embedding(self, text: str):
        response = self.openai_client.embeddings.create(
            input=[text], model=self.embedding_model
        )
        return response.data[0].embedding

    def load_documents_from_folder(self):
        all_docs = []
        for filename in os.listdir(self.folder_path):
            if filename.endswith(".json"):
                filepath = os.path.join(self.folder_path, filename)
                with open(filepath, "r", encoding="utf-8") as f:
                    try:
                        data = json.load(f)
                        all_docs.extend(data)
                        print(f"[INFO] {filename} â†’ {len(data)}ê±´ ë¡œë“œ")
                    except Exception as e:
                        print(f"[ERROR] {filename} ì‹¤íŒ¨: {e}")
        return all_docs

    def build_or_load_index(self, force_rebuild=False):
        if (
            not force_rebuild
            and os.path.exists(self.index_path)
            and os.path.exists(self.meta_path)
        ):
            print("[INFO] ì €ì¥ëœ FAISS ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì¤‘...")
            self.index = faiss.read_index(self.index_path)
            with open(self.meta_path, "r", encoding="utf-8") as f:
                meta = json.load(f)
                self.doc_ids.extend(meta["ids"])
                self.doc_texts.extend(meta["texts"])
                self.doc_meta.extend(meta["meta"])
            return self.index

        print("[INFO] íŒŒì¼ë¡œë“œì¤‘..")
        documents = self.load_documents_from_folder()
        self.doc_texts = [doc["content"] for doc in documents]
        self.doc_ids = [doc.get("id", idx) for idx, doc in enumerate(documents)]
        self.doc_meta = [
            {
                "title": doc.get("title", ""),
                "url": doc.get("url", ""),
                "thumbNailUrl": doc.get("thumbNailUrl", ""),
                "category": doc.get("category", ""),
            }
            for doc in documents
        ]
        print("[INFO] embeddingì¤‘...")
        embeddings = []
        for i, text in enumerate(self.doc_texts):
            emb = self.get_openai_embedding(text)
            embeddings.append(emb)
            time.sleep(0.5)
            print(f"[{i+1}/{len(self.doc_texts)}] ì„ë² ë”© ì™„ë£Œ")

        embeddings = np.array(embeddings).astype("float32")

        dim = embeddings.shape[1]
        self.index = faiss.IndexFlatL2(dim)
        self.index.add(embeddings)

        np.save(self.embed_path, embeddings)
        faiss.write_index(self.index, self.index_path)
        with open(self.meta_path, "w", encoding="utf-8") as f:
            json.dump(
                {"ids": self.doc_ids, "texts": self.doc_texts, "meta": self.doc_meta},
                f,
                ensure_ascii=False,
            )

        print("[INFO] ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ")
        return self.index

    def search(self, query: str, top_k: int = 3):
        if self.index is None:
            raise ValueError(
                "FAISS ì¸ë±ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € build_or_load_index()ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”."
            )

        query_vec = np.array([self.get_openai_embedding(query)]).astype("float32")
        D, I = self.index.search(query_vec, top_k)

        results = []
        for i in I[0]:
            idx = int(i)
            results.append(
                {
                    "id": self.doc_ids[idx],
                    "content": self.doc_texts[idx],
                    "title": self.doc_meta[idx]["title"],
                    "url": self.doc_meta[idx]["url"],
                    "thumbNailUrl": self.doc_meta[idx]["thumbNailUrl"],
                    "category": self.doc_meta[idx]["category"],
                }
            )
        return results


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    load_dotenv()

    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    recommender = RAGBlogRecommender()
    # recommender.initialize_system()

    # ë©”ëª¨ì—ì„œ ì¶”ì¶œí•œ ë°ì´í„°
    momo_result = """
        {
        "contents": "ì˜¤ëŠ˜ ì‚¬ë‚´ AI ì›Œí¬ìˆì—ì„œ ìµœì‹  ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ  ë™í–¥ íŒŒì•…. ì‹¤ì‹œê°„ í† ë¡  ì¤‘ ì†Œí†µ...",
        "interests": ["AI íŠ¸ë Œë“œ", "ê¸°ìˆ  ë™í–¥", "ì§ì—… ê°œë°œ"],
        "pain_points": ["ëŒ€ì¸ ê´€ê³„", "ì†Œí†µ ë¬¸ì œ", "ì™¸ë¡œì›€"],
        "style": "ë¶„ì„ì ì¸",
        "ìµœì‹ ì„±": 1.0,
        "ì‹ ë¢°ì„±": 1.0,
        "ì‹¤ìš©ì„±": 1.0,
        "ì•„ì¹´ì´ë¸Œì„±": 1.0,
        "ë‹¤ì–‘ì„±": 0.5,
        "ë¶„ì„ì„±": 1.0,
        "category": "cautious_type"
        }
    """

    # LLMì„ í†µí•œ ì„±í–¥ ì¹´í…Œê³ ë¦¬, ì‚¬ìš©ì ì„±í–¥, í”„ë¡¬í”„íŠ¸ë¥¼ ë½‘ëŠ”ë‹¤.
    response = recommender.categorize_memo(momo_result)
    print(response)

    prompt = recommender.extract_json_prompt(response)
    print("ğŸ¯ Prompt:", prompt)

    recommender = RAGBlogRecommender(
        folder_path="./new_naver_crawl/crawl_new_data2",
        # api_key="sk-...",  # ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •
    )

    index = recommender.build_or_load_index(
        force_rebuild=False
    )  # force_rebuild=True ê°€ëŠ¥

    # query = """ì‚¬ìš©ìê»˜ì„œëŠ” ITì™€ ì»´í“¨í„° ë¶„ì•¼ì— ë†’ì€ ê´€ì‹¬ì„ ê°€ì§€ê³  ê³„ì‹­ë‹ˆë‹¤. ìµœì‹  ê¸°ìˆ  ë™í–¥, IT íŠ¸ë Œë“œ..."""

    results_all = []

    for q in prompt:
        print(f"{q}")
        results_all += recommender.search(q)

    print("\nğŸ” ìœ ì‚¬ ë¬¸ì„œ:")
    for r in results_all:
        print(f"- ID: {r['id']}")
        print(f"  ğŸ”¹ Title: {r['title']}")
        print(f"  ğŸ”¹ Category: {r['category']}")
        print(f"  ğŸ”¹ URL: {r['url']}")
        print(f"  ğŸ”¹ thumbNailUrl: {r['thumbNailUrl']}")
        print(f"  ğŸ”¹ Content: {r['content'][:500]}...\n")


if __name__ == "__main__":
    main()
