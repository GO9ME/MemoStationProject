import json
import faiss
import numpy as np
import time
import os
import re
import nltk
import hashlib
from datetime import datetime

from pathlib import Path
from dotenv import load_dotenv
from typing import List, Dict, Optional, Tuple
from nltk.corpus import stopwords
from tqdm import tqdm
from openai import OpenAI


# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ
try:
    nltk.download("punkt", quiet=True)
    nltk.download("stopwords", quiet=True)
except:
    pass


class RAGBlogRecommender:
    def __init__(
        self,
        folder_path: str = "new_naver_crawl/crawl_new_data2",
        # folder_path: str,
        embedding_model: str = "text-embedding-3-small",
        index_path: str = "faiss_index_basic.faiss",
        embed_path: str = "embeddings_basic.npy",
        meta_path: str = "doc_metadata_basic.json",
        api_key: str = None,
    ):

        self.folder_path = folder_path
        self.embedding_model = embedding_model
        self.index_path = index_path
        self.embed_path = embed_path
        self.meta_path = meta_path

        self.doc_texts = []
        self.doc_ids = []
        self.doc_meta = []
        self.index = None

        """
        RAG ê¸°ë°˜ ë¸”ë¡œê·¸ ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”

        Args:
            blog_data_path: ë¸”ë¡œê·¸ í¬ë¡¤ë§ ë°ì´í„° ê²½ë¡œ
            api_key: OpenAI API í‚¤
        """
        self.blog_data_path = Path(folder_path)
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")

        if not self.api_key:
            raise ValueError("OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        self.openai_client = OpenAI(api_key=self.api_key)

        # í•œêµ­ì–´/ì˜ì–´ ë¶ˆìš©ì–´ ì„¤ì •
        self.stop_words = set(stopwords.words("english"))
        try:
            self.stop_words.update(stopwords.words("korean"))
        except:
            pass

        # ë¸”ë¡œê·¸ ë°ì´í„° ì €ì¥ì†Œ
        self.blog_data = []
        self.blog_embeddings = None
        self.blog_tfidf_matrix = None

    def preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if not text:
            return ""

        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r"<[^>]+>", "", text)

        # íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±ë§Œ ìœ ì§€)
        text = re.sub(r"[^\w\sê°€-í£]", " ", text)

        # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = re.sub(r"\s+", " ", text)

        # ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()

        return text

    def extract_json_prompt(self, text: str) -> str:
        """
        LLM ì‘ë‹µ ë¬¸ìì—´ì—ì„œ JSON ë¸”ëŸ­ì„ ì¶”ì¶œí•˜ê³  'prompt' ê°’ë§Œ ë°˜í™˜
        """
        # ì½”ë“œë¸”ëŸ­ ì œê±°: ```json ë˜ëŠ” ``` ìœ¼ë¡œ ê°ì‹¸ì§„ ë¶€ë¶„ ì‚­ì œ
        cleaned = re.sub(
            r"^```json|^```|```$", "", text.strip(), flags=re.MULTILINE
        ).strip()

        # JSON íŒŒì‹±
        try:
            data = json.loads(cleaned)
            return data.get("prompt", "")
        except json.JSONDecodeError as e:
            print("âŒ JSON íŒŒì‹± ì˜¤ë¥˜:", e)
            print("ì…ë ¥ê°’:", cleaned[:200])
            return ""

    def categorize_memo(self, memo_text: str, n: int = 5) -> str:
        # LLM ë¶„ë¥˜ëŠ” ì„ íƒì ìœ¼ë¡œ ì‚¬ìš© (API ì˜¤ë¥˜ ë°©ì§€)
        try:

            prompt = f"""

ì•„ë˜ì— ì œì‹œëœ í‚¤ì›Œë“œ ê·¸ë£¹ë“¤ì„ ì°¸ê³ í•˜ì—¬, ê° ê·¸ë£¹ë§ˆë‹¤ **ë¸”ë¡œê·¸ ì½˜í…ì¸  ê²€ìƒ‰ìš© ì¿¼ë¦¬ ë¬¸ì¥**ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.  
ê° ì¿¼ë¦¬ëŠ” ë‹¤ìŒ ê¸°ì¤€ì„ ë”°ë¦…ë‹ˆë‹¤:            

### ë©”ëª¨ ê¸°ë°˜ ì„±í–¥ ë°ì´í„°
{memo_text}

## [ì§€ì‹œì‚¬í•­: ì…ë ¥ê°’ì— ë”°ë¼ ìœ ë™ì ìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”]
1. ëª©ì : ì‚¬ìš©ìê°€ í¥ë¯¸ë¡œì›Œí•  ë¸”ë¡œê·¸ ê¸€ì„ ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰í•˜ê¸° ìœ„í•¨
2. í˜•ì‹: `í‚¤ì›Œë“œ ê·¸ë£¹ ê¸°ë°˜ ìì—°ì–´ ì¿¼ë¦¬ ë¬¸ì¥` 5ê°œ (ê·¸ë£¹ 0~4)
3. í†¤: êµ¬ì²´ì ì´ê³  ì •ë³´ ì¤‘ì‹¬
4. ê¸¸ì´: 1ë¬¸ì¥ ~ 2ë¬¸ì¥ ì´ë‚´
5. ì •ë³´ ì†Œë¹„ ê°€ì¤‘ì¹˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒì„ ë°˜ì˜í•˜ì„¸ìš”(ë©”ëª¨ê¸°ë°˜ ì„±í–¥ ë°ì´í„°ì— ë”°ë¼ ìœ ë™ì ìœ¼ë¡œ ë³€í•´ì•¼ í•¨):
   - **ì‹ ë¢°ì„±(credibility)** > **ë‹¤ì–‘ì„±(diversity)** > **ë¶„ì„ì„±(analyticity)** > **ì‹¤ìš©ì„±(utility)** > **ë³µìŠµì„±(archive)** ìˆœì„œë¡œ ì •ë³´ ì„ í˜¸
   - **ìµœì‹ ì„±(recency)**ì€ ë§¤ìš° ë‚®ìœ¼ë¯€ë¡œ ë°˜ì˜í•˜ì§€ ì•Šì•„ë„ ë¬´ë°©
6. ì¶”ì²œí•  ë¸”ë¡œê·¸ ì¹´í…Œê³ ë¦¬ë„ ë¸”ë¡œê·¸ categoryë¥¼ ì°¸ì¡°í•˜ì—¬ í•¨ê»˜ ì œì•ˆí•˜ì„¸ìš”. ì˜ˆ:
   - `"ì§€ì‹ë™í–¥ > ITÂ·ì»´í“¨í„°"`  
   - `"ìƒí™œë…¸í•˜ìš°ì‡¼í•‘ > ì¼ìƒìƒê°"`  
7. êµ¬ì„± ì˜ˆì‹œ: `"ìµœê·¼ [í‚¤ì›Œë“œ]ì™€ ê´€ë ¨ëœ ì‹¤ìš©ì ì¸ ì •ë³´ë¥¼ ë‹¤ë£¬ ë¸”ë¡œê·¸ ê¸€ ì¶”ì²œí•´ì¤˜"`  
  ë˜ëŠ” `"ì‚¬ìš©ìì˜ ê´€ì‹¬ì‚¬ë¥¼ ê³ ë ¤í•˜ì—¬ [í‚¤ì›Œë“œ]ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•œ ì½˜í…ì¸ ë¥¼ ì°¾ê¸° ìœ„í•œ ì¿¼ë¦¬"`
8. ì •ë³´ ì†Œë¹„ ê°€ì¤‘ì¹˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°€ì¤‘ì¹˜ê°€ í° ìˆœì„œëŒ€ë¡œ ë‚˜ì—´í•˜ì‹œì˜¤

## [ë¸”ë¡œê·¸ category:
# 1. ì§€ì‹ë™í–¥: ê±´ê°•ì˜í•™, êµìœ¡í•™ë¬¸, ë¹„ì¦ˆë‹ˆìŠ¤ê²½ì œ, ì‚¬íšŒì •ì¹˜, ì–´í•™ì™¸êµ­ì–´, ITÂ·ì»´í“¨í„°
# 2. ì·¨ë¯¸ì—¬ê°€ì—¬í–‰: ê²Œì„, êµ­ë‚´ì—¬í–‰, ë§›ì§‘, ì‚¬ì§„, ì„¸ê³„ì—¬í–‰, ìŠ¤í¬ì¸ , ìë™ì°¨, ì·¨ë¯¸
# 3. ìƒí™œë…¸í•˜ìš°ì‡¼í•‘: ë°˜ë ¤ë™ë¬¼, ìƒí’ˆë¦¬ë·°, ìš”ë¦¬ë ˆì‹œí”¼, ì›ì˜ˆì¬ë°°, ìœ¡ì•„ê²°í˜¼, ì¸í…Œë¦¬ì–´DIY, ì¼ìƒìƒê°, ì¢‹ì€ê¸€ì´ë¯¸ì§€, í´ì…˜ë¯¸ìš©
# 4. ì—”í„°í…Œì¸ë¨¼íŠ¸ì˜ˆìˆ : ê³µì—°ì „ì‹ , ë“œë¼ë§ˆ, ë§Œí™”ì• ë‹ˆ, ë¬¸í•™ì±…, ë¯¸ìˆ ë””ìì¸, ë°©ì†¡, ìŠ¤íƒ€ì—°ì˜ˆì¸, ì˜í™”, ìŒì•…

# â€» ì–´í•™/í•™ìŠµ/ê³µë¶€/í‘œí˜„/ì˜ì–´/ì™¸êµ­ì–´ ê´€ë ¨ ë‹¨ì–´ê°€ í•˜ë‚˜ë¼ë„ ë‚˜ì˜¤ë©´ ë°˜ë“œì‹œ 'ì§€ì‹ë™í–¥'ìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
# â€» ìŒì‹/ìš”ë¦¬/ë ˆì‹œí”¼ ê´€ë ¨ ë‹¨ì–´ê°€ ë‚˜ì˜¤ë©´ ë°˜ë“œì‹œ 'ìƒí™œë…¸í•˜ìš°ì‡¼í•‘'ìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
# â€» ê²Œì„/RPG/í”Œë ˆì´ ê´€ë ¨ ë‹¨ì–´ê°€ ë‚˜ì˜¤ë©´ ë°˜ë“œì‹œ 'ì·¨ë¯¸ì—¬ê°€ì—¬í–‰'ìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
# â€» ìœ„ì˜ ì¹´í…Œê³ ë¦¬ì— ëª…í™•íˆ í•´ë‹¹í•˜ì§€ ì•Šì„ ë•ŒëŠ” ê°€ì¥ ê·¼ì ‘í•œ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš”. 'ê¸°íƒ€'ëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.   

## [category ë³„ ì •ì˜]
# íŠ¸ë Œë“œ ì¶”ì¢…í˜•(trend_type) : ìµœì‹  ì •ë³´, ì‹ ì œí’ˆ, ì‹¤ì‹œê°„ ì´ìŠˆ, ë¹ ë¥¸ ë³€í™”ì— ë¯¼ê°. ì •ë³´ì˜ ì‹ ì„ ë„ì™€ ì†ë„ ì¤‘ì‹œ
# ì‹ ì¤‘ ê²€ì¦í˜•(cautious_type):  ê²€ì¦ëœ ì •ë³´, ì „ë¬¸ê°€ ì˜ê²¬, ë¦¬ë·°, ì˜¤ëœ ê²½í—˜ ì¤‘ì‹œ. ì‹ ë¢°ì„±, ê·¼ê±°, ë°˜ë³µì„± ê°•ì¡°
# ê· í˜•í˜•(balanced_type): ìµœì‹ ì„±ê³¼ ê²€ì¦ì„±ì„ ìƒí™©ì— ë”°ë¼ ì¡°í™”ë¡­ê²Œ ì†Œë¹„. ìƒˆë¡œì›€ê³¼ ì‹ ë¢°ì„± ëª¨ë‘ ì¼ì • ë¹„ìœ¨ ê³ ë ¤
# íšŒê³ /ë³µìŠµí˜•(retrospective_type): ê³¼ê±° ê¸°ë¡, ë³µìŠµ, ë°˜ë³µ í•™ìŠµ, ì•„ì¹´ì´ë¸Œ ì •ë³´ ì„ í˜¸. ëˆ„ì ì„±, ë°˜ë³µì„±, ì¥ê¸° ê°€ì¹˜ ì¤‘ì‹œ
# ì‹¤ìš©/ì¦‰ì‹œí˜•(practical_type): ë‹¹ì¥ ì“¸ ìˆ˜ ìˆëŠ” ì‹¤ìš© ì •ë³´, How-to, ìš”ì•½, íŒ ì„ í˜¸. ì ìš© ê°€ëŠ¥ì„±, ì¦‰ì‹œì„± ê°•ì¡°

## [query ì˜ˆì‹œ]
"ì•„ì¹¨ ë£¨í‹´ì´ë‚˜ ë°”ì´ì˜¤ ë¦¬ë“¬ì²˜ëŸ¼ ìƒí™œì˜ ì›€ì§ì„ê³¼ ê´€ë ¨ëœ ì‹¤ìš©ì ì¸ íŒì„ ë‹´ì€ ë¸”ë¡œê·¸ ê¸€ì„ ì°¾ì•„ì¤˜"
"í•´ì™¸ ì‹œì¥ ë™í–¥ì´ë‚˜ ê¸°ì—…ì˜ ë³€í™” ì˜ˆì¸¡ê³¼ ê°™ì€ ì‹¤ìš©ì ì¸ ì •ë³´ë¥¼ ë‹¤ë£¬ ë¸”ë¡œê·¸ ê¸€ì„ ì¶”ì²œí•´ì¤˜"
"ì˜í™” ì† ìë™í™” ê¸°ìˆ ì´ë‚˜ í™˜ê²½ ë³€í™”ì— ëŒ€í•´ ë‹¤ë£¬ ë¶„ì„ì ì¸ ë¸”ë¡œê·¸ ê¸€ì„ ê²€ìƒ‰í•´ì¤˜"
"AI, UX, ì œí’ˆ ë””ìì¸ê³¼ ê´€ë ¨ëœ ìµœì‹  ê¸°ìˆ  íŠ¸ë Œë“œë¥¼ ì†Œê°œí•œ ë¸”ë¡œê·¸ ì½˜í…ì¸ ë¥¼ ì¶”ì²œí•´ì¤˜"
"ìš°ì •, ì•ˆì •ê°, ìƒì¡´ ì´ìŠˆì²˜ëŸ¼ ì¸ê°„ê´€ê³„ë‚˜ ì‚¬íšŒì  ê°€ì¹˜ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•œ ì½˜í…ì¸ ë¥¼ ì°¾ì•„ì¤˜"

## [ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ]

```json

  "user_category": "balanced_type",
  "user_intent_summary": "ì‚¬ìš©ìëŠ” ìµœì‹ ì„±ê³¼ ì‹ ë¢°ì„±ì„ ëª¨ë‘ ì¤‘ìš”ì‹œí•˜ë©°, ë¶„ì„ì ì´ê³  ì‹¤ìš©ì ì¸ ì½˜í…ì¸ ë¥¼ ì„ í˜¸í•©ë‹ˆë‹¤. ìŒì•…ê³¼ ì˜¤ë””ì˜¤ ê¸°ìˆ , ì§ì¥ ë‚´ íš¨ìœ¨ì„± ê´€ë ¨ ì£¼ì œë¥¼ ìì£¼ ë‹¤ë£¹ë‹ˆë‹¤.",
  "category": ["ì§€ì‹ë™í–¥ > IT, ì»´í“¨í„°", "ìƒí™œë…¸í•˜ìš°ì‡¼í•‘ > ì¼ìƒ"],
  "prompt": ["queryì˜ ê°¯ìˆ˜ì— ë”°ë¼ ìœ ë™ì ìœ¼ë¡œ ë³€í•¨"],
  "info_weight" : "ì‹ ë¢°ì„±, ë‹¤ì–‘ì„± .. "
  
                       """
            print(f" prompt : {prompt}")

            system_prompt = f"""
                ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ê´€ì‹¬ì‚¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìœ ì‚¬í•œ ë¸”ë¡œê·¸ ê¸€ì„ ê²€ìƒ‰í•˜ê¸° ìœ„í•œ ì¿¼ë¦¬ë¥¼ ë§Œë“œëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.
            """
            print(f"[INFO]system_prompt : ==== {system_prompt}")
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system",
                        "content": system_prompt,
                    },
                    {"role": "user", "content": prompt},
                ],
                temperature=0.3,
            )

            llm_category = response.choices[0].message.content.strip().strip('"')

            return llm_category

        except Exception as e:
            print(f"LLM ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # LLM ì˜¤ë¥˜ ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ ê²°ê³¼ ì‚¬ìš©

    def get_openai_embedding(self, text: str):
        response = self.openai_client.embeddings.create(
            input=[text], model=self.embedding_model
        )
        return response.data[0].embedding

    def compute_meta_score(self, meta: Dict) -> float:
        return (
            0.3 * meta.get("credibility", 0)
            + 0.2 * meta.get("recency", 0)
            + 0.2 * meta.get("utility", 0)
            + 0.2 * meta.get("diversity", 0)
            + 0.1 * meta.get("analyticity", 0)
        )

    def load_documents_from_folder(self):
        all_docs = []
        for filename in os.listdir(self.folder_path):
            if filename.endswith(".json"):
                filepath = os.path.join(self.folder_path, filename)
                with open(filepath, "r", encoding="utf-8") as f:
                    try:
                        data = json.load(f)
                        for d in data:
                            d["credibility"] = float(d.get("credibility", 0))
                            d["recency"] = float(d.get("recency", 0))
                            d["utility"] = float(d.get("utility", 0))
                            d["diversity"] = float(d.get("diversity", 0))
                            d["analyticity"] = float(d.get("analyticity", 0))
                        all_docs.extend(data)
                        print(f"[INFO] {filename} â†’ {len(data)}ê±´ ë¡œë“œ")
                    except Exception as e:
                        print(f"[ERROR] {filename} ì‹¤íŒ¨: {e}")
        return all_docs

    def build_or_load_index(self, force_rebuild=False):
        if (
            not force_rebuild
            and os.path.exists(self.index_path)
            and os.path.exists(self.meta_path)
        ):
            print("[INFO] ì €ì¥ëœ FAISS ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì¤‘...")
            self.index = faiss.read_index(self.index_path)
            with open(self.meta_path, "r", encoding="utf-8") as f:
                meta = json.load(f)
                self.doc_ids = meta["ids"]
                self.doc_texts = meta["texts"]
                self.doc_meta = meta["meta"]
            return self.index

        print("[INFO] ìƒˆë¡œ ë¬¸ì„œ ë¡œë“œ ì¤‘...")
        documents = self.load_documents_from_folder()
        self.doc_texts = [doc["content"] for doc in documents]
        self.doc_ids = [
            doc.get("id", hashlib.md5(doc["content"].encode()).hexdigest())
            for doc in documents
        ]
        self.doc_meta = [
            {
                "title": doc.get("title", ""),
                "url": doc.get("url", ""),
                "thumbNailUrl": doc.get("thumbNailUrl", ""),
                "category": doc.get("category", ""),
                # ì‹ ë¢°ì„± "ì „ë¬¸ê°€" "ì¶œì²˜"
                "credibility": doc.get("credibility", 0),
                # ìµœì‹ ì„±(ë‚ ì§œ)
                "recency": doc.get("recency", 0),
                # "íŒ", "ì •ë¦¬", "ë°©ë²•", "ìš”ì•½"
                "utility": doc.get("utility", 0),
                # ë‹¤ì–‘ì„± ë‹¨ì–´ì˜ ë‹¤ì–‘ì„±
                "diversity": doc.get("diversity", 0),
                # ë¶„ì„ì„± "ë¶„ì„", "ê²°ê³¼", "ì´ìœ ", "íŒ¨í„´"
                "analyticity": doc.get("analyticity", 0),
            }
            for doc in documents
        ]

        print("[INFO] ì„ë² ë”© ì¤‘...")
        embeddings = []
        for i, text in enumerate(self.doc_texts):
            emb = self.get_openai_embedding(text)
            embeddings.append(emb)
            print(f"[{i+1}/{len(self.doc_texts)}] ì„ë² ë”© ì™„ë£Œ")
            time.sleep(0.3)  # OpenAI í˜¸ì¶œ ì‹œ ì œí•œ ê³ ë ¤

        embeddings = np.array(embeddings).astype("float32")
        self.index = faiss.IndexFlatL2(embeddings.shape[1])
        self.index.add(embeddings)

        np.save(self.embed_path, embeddings)
        faiss.write_index(self.index, self.index_path)
        with open(self.meta_path, "w", encoding="utf-8") as f:
            json.dump(
                {"ids": self.doc_ids, "texts": self.doc_texts, "meta": self.doc_meta},
                f,
                ensure_ascii=False,
            )
        print("[INFO] ì¸ë±ìŠ¤ ë° ë©”íƒ€ ì €ì¥ ì™„ë£Œ")

    def search(self, query: str, top_k: int = 3, alpha: float = 0.5):
        if self.index is None:
            raise ValueError("FAISS ì¸ë±ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        query_vec = np.array([self.get_openai_embedding(query)]).astype("float32")
        D, I = self.index.search(query_vec, top_k * 3)

        results = []
        for i in I[0]:
            idx = int(i)
            if idx >= len(self.doc_ids):
                continue
            meta = self.doc_meta[idx]
            vector_score = 1 / (1 + D[0][list(I[0]).index(i)])  # L2 ê±°ë¦¬ â†’ ìœ ì‚¬ë„
            meta_score = self.compute_meta_score(meta)
            final_score = alpha * vector_score + (1 - alpha) * meta_score
            results.append(
                {
                    "id": self.doc_ids[idx],
                    "content": self.doc_texts[idx],
                    "title": meta.get("title", ""),
                    "url": meta.get("url", ""),
                    "thumbNailUrl": meta.get("thumbNailUrl", ""),
                    "category": meta.get("category", ""),
                    "score": round(final_score, 4),
                }
            )

        results.sort(key=lambda x: x["score"], reverse=True)
        return results[:top_k]


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    load_dotenv()

    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    recommender = RAGBlogRecommender()

    # ë©”ëª¨ì—ì„œ ì¶”ì¶œí•œ ì¶”ì²œ ë°ì´í„°
    memo_result = """
{
    "user_id": "ì‚¬ìš©ì_012",
    "memo_id": [
      "de88e2a0-5954-4ab3-96ab-46f9391079ea",
      "e80519ad-03d3-4ea6-b3cc-605f728b397a",
      "7f7e7f02-be28-498f-bdd2-e7b92f69d097",
      "94a109d8-290c-410d-afa6-9c520e5b4181",
      "ae9383c8-a521-4259-9a40-c9cf168a448e",
      "dcced74a-0556-445b-8465-c97bd012a540",
      "6940a3a6-9cf8-4f7d-905a-63ec5993542c",
      "b16c561c-fa62-4a61-afec-985348657634",
      "b7ac06f5-f6f1-4399-b123-648050524bbf",
      "f599af60-6b1d-4f45-aa2d-4023ab11c3b0",
      "83bd22d0-c3ee-4ee2-a2ab-ba51d010e344",
      "48344ece-2a9b-4595-ab8d-3e4db80364d3",
      "570e15cf-dec2-4dd3-bf02-fdfbd97f23b1",
      "cd833753-3c4f-4474-af6d-3cd7ec6e76e2",
      "efef251a-d886-4a83-9361-7cfcab4445b6",
      "06773fbf-4a30-4a60-8943-01278c3c5365",
      "f418141f-786a-495a-8ffe-06d2e9598f58",
      "8a543cc1-607a-42fa-8026-cdcc1aa8aa9e",
      "d566b32f-e041-44bc-bc8a-3d7561bd539f",
      "9fd361df-709e-464b-be14-928f8b764dd0"
    ],
    "recommend_value": {
      "keyword": {
        "0": [
          "ì´ë¡ ",
          "ì˜¨ë¼ì¸",
          "ì´ë™",
          "ì‹œì¥",
          "ì‹¤ì „",
          "ì¸ì§€",
          "ì¸ìƒ",
          "ì´ë¥¸",
          "ì§„ë¡œ",
          "ì´í•´"
        ],
        "1": [
          "ìƒê°",
          "ì—¬í–‰",
          "ê³µê°•",
          "ë°©ì‹",
          "ìê²©ì¦",
          "ê°•ì¡°",
          "ê°€ì¥",
          "ê°•ì—°",
          "ì¤‘ê°„ê³ ì‚¬",
          "ì‘ì„±"
        ],
        "2": [
          "ì²´í—˜",
          "ë¼ì´íŠ¸",
          "ë§ˆì´í¬",
          "í”¼ë¡œê°",
          "ì´ˆì—¬ë¦„",
          "ì–´ë ¤ìš°",
          "ì…ê°",
          "ì—°êµ¬",
          "ì±•í„°",
          "ì¶”ì²œ"
        ],
        "3": [
          "task",
          "q",
          "school",
          "work",
          "career",
          "time",
          "life",
          "design",
          "keyword",
          "teamwork"
        ],
        "4": [
          "ë¶™ì¡",
          "ì·¨ì—…",
          "ì–‘ë³´",
          "í•´ì•¼",
          "ì ˆì•½",
          "ì„¤ê³„",
          "ëíŒ",
          "ì²­ì·¨",
          "ìœ„í•´",
          "ë‹¨ìœ„"
        ]
      },
      "forgetting_curve_weight": 0.457160751985381,
      "recency": 0.15143227660774675,
      "credibility": 0.26604827665160324,
      "utility": 0.14226336154766567,
      "archive": 0.1964882140340277,
      "diversity": 0.052423400160111946,
      "analyticity": 0.19134447099884475,
      "category": "retrospective_type"
    }
  }
    """

    # LLMì„ í†µí•œ ì„±í–¥ ì¹´í…Œê³ ë¦¬, ì‚¬ìš©ì ì„±í–¥, í”„ë¡¬í”„íŠ¸ë¥¼ ë½‘ëŠ”ë‹¤.
    response = recommender.categorize_memo(memo_result)
    print("ğŸ¯ response:", response)

    prompt = recommender.extract_json_prompt(response)
    print("ğŸ¯ Prompt:", prompt)

    recommender = RAGBlogRecommender(
        folder_path="./new_naver_crawl/crawl_new_data2",
        # api_key="sk-...",  # ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •
    )

    index = recommender.build_or_load_index(
        force_rebuild=False
    )  # force_rebuild=True ê°€ëŠ¥

    # query = """ì‚¬ìš©ìê»˜ì„œëŠ” ITì™€ ì»´í“¨í„° ë¶„ì•¼ì— ë†’ì€ ê´€ì‹¬ì„ ê°€ì§€ê³  ê³„ì‹­ë‹ˆë‹¤. ìµœì‹  ê¸°ìˆ  ë™í–¥, IT íŠ¸ë Œë“œ..."""

    results_all = []

    for q in prompt:
        print(f"{q}")
        results_all += recommender.search(q)

    # âœ… ID ê¸°ì¤€ ì¤‘ë³µ ì œê±° í•¨ìˆ˜
    def deduplicate_by_id(items: list[dict], id_key: str = "id") -> list[dict]:
        seen = set()
        unique = []
        for item in items:
            item_id = item.get(id_key) or item.get("ID")  # ìœ ì—°í•˜ê²Œ ì²˜ë¦¬
            if item_id and item_id not in seen:
                seen.add(item_id)
                unique.append(item)
        return unique

    # âœ… ì¤‘ë³µ ì œê±° ì ìš©
    results_all = deduplicate_by_id(results_all)
    print(results_all)

    # JSON íŒŒì¼ë¡œ ì €ì¥
    # with open("./user_test/blogs_user_010.json", "w", encoding="utf-8") as f:
    #     json.dump(list(results_all), f, ensure_ascii=False, indent=2)
    with open("./user_test/blogs_user_012.json", "w", encoding="utf-8") as f:
        json.dump(results_all, f, ensure_ascii=False, indent=2, default=str)

    print("âœ… ê²€ìƒ‰ ê²°ê³¼ê°€ blogs_user_012.jsonìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # scoreì— ì˜í•œ ì •ë ¬í•˜ê¸°
    # results_all.sort(key=lambda x: x["score"], reverse=True)

    print("\nğŸ” ìœ ì‚¬ ë¬¸ì„œ:")
    for i, r in enumerate(results_all, start=1):
        print(f"- ID: {i}. {r['id']}")
        print(f"  ğŸ”¹ Title: {r['title']}")
        print(f"  ğŸ”¹ Category: {r['category']}")
        print(f"  ğŸ”¹ URL: {r['url']}")
        print(f"  ğŸ”¹ thumbNailUrl: {r['thumbNailUrl']}")
        print(f"  ğŸ”¹ Content: {r['content'][:500]}...\n")
        print(f"  ğŸ”¹ score: {r['score']}...\n")


if __name__ == "__main__":
    main()
